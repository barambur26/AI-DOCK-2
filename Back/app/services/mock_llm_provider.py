# Mock LLM Provider for Development and Testing
# This provider simulates LLM responses without making external API calls

import asyncio
import time
import random
from typing import Dict, Any, Optional
from datetime import datetime

from .llm_service import BaseLLMProvider, ChatRequest, ChatResponse, ChatMessage

class MockLLMProvider(BaseLLMProvider):
    """
    Mock LLM provider for development and testing.
    
    This provider simulates realistic LLM responses without making external API calls.
    Perfect for:
    - Development when API quotas are exceeded
    - Testing without incurring costs
    - Demonstrating the application's functionality
    - Learning LLM integration patterns
    """
    
    @property
    def provider_name(self) -> str:
        return "Mock Provider (Development)"
    
    # Collection of realistic mock responses
    MOCK_RESPONSES = [
        "Hello! I'm a mock LLM response. This simulates how a real AI assistant would respond to your message.",
        "I understand you're testing the AI Dock application. This is a simulated response that demonstrates the chat functionality without using external APIs.",
        "This is a mock response from the development LLM provider. In a real deployment, this would be generated by OpenAI, Claude, or another LLM service.",
        "Great question! I'm simulating an AI response here. This mock provider helps developers test the application without API costs or quota limits.",
        "I'm a simulated AI assistant response. This allows you to test the full chat interface and user experience while development is in progress.",
        "This mock response demonstrates how the AI Dock handles LLM interactions. You can see message flow, UI updates, and response formatting in action!",
        "Perfect! Your chat interface is working correctly. This simulated response shows that messages are being processed through the backend service layer.",
        "Excellent work on building this AI gateway! This mock provider lets you test all features without external dependencies or API costs.",
    ]
    
    async def send_chat_request(self, request: ChatRequest) -> ChatResponse:
        """
        Simulate sending a chat request to an LLM.
        
        This method:
        1. Validates the request (just like a real provider)
        2. Simulates realistic response time
        3. Generates contextual mock responses
        4. Returns properly formatted ChatResponse
        """
        self._validate_configuration()
        
        # Simulate realistic API response time (200-1500ms)
        response_time_ms = random.randint(200, 1500)
        await asyncio.sleep(response_time_ms / 1000)
        
        # Get the last user message for context
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        last_message = user_messages[-1].content.lower() if user_messages else ""
        
        # Generate contextual response
        mock_content = self._generate_contextual_response(last_message)
        
        # Simulate realistic token usage
        # Input tokens = roughly message length / 4
        input_tokens = sum(len(msg.content) for msg in request.messages) // 4
        
        # Output tokens = response length / 4  
        output_tokens = len(mock_content) // 4
        
        usage = {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens
        }
        
        # Simulate cost calculation (much cheaper than real APIs!)
        cost = self._calculate_actual_cost(usage) or 0.001  # Minimal mock cost
        
        return ChatResponse(
            content=mock_content,
            model=request.model or self.config